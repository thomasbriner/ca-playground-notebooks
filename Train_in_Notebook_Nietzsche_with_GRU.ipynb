{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\"https://www.gutenberg.org/files/7205/7205-0.txt\",\n",
    "      \"http://www.gutenberg.org/cache/epub/7204/pg7204.txt\",\n",
    "      \"http://www.gutenberg.org/cache/epub/7202/pg7202.txt\",\n",
    "      \"https://www.gutenberg.org/files/60360/60360-0.txt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for url in urls:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        text += f.read().decode('utf-8')[900:-13000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1868534"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Buch für Alle und Keinen\n",
      "\n",
      "\n",
      "von Friedrich Wilhelm Nietzsche\n",
      "\n",
      "\n",
      "\n",
      "Inhaltsverzeichnis\n",
      "\n",
      "\n",
      " Erster Theil\n",
      " Zarathustra’s Vorrede\n",
      " Die Reden Zarathustra’s\n",
      " Von den drei Verwandlungen\n",
      " Von den Lehrstühlen der Tugend\n",
      " Von den Hinterweltlern\n",
      " V\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '\"' :   4,\n",
      "  '#' :   5,\n",
      "  '&' :   6,\n",
      "  \"'\" :   7,\n",
      "  '(' :   8,\n",
      "  ')' :   9,\n",
      "  '*' :  10,\n",
      "  ',' :  11,\n",
      "  '-' :  12,\n",
      "  '.' :  13,\n",
      "  '/' :  14,\n",
      "  '0' :  15,\n",
      "  '1' :  16,\n",
      "  '2' :  17,\n",
      "  '3' :  18,\n",
      "  '4' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'in Buch für A' ---- characters mapped to int ---- > [ 68  73   2  31  80  62  67   2  65 108  77   2  30]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "n\n",
      " \n",
      "B\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'in Buch für Alle und Keinen\\r\\n\\r\\n\\r\\nvon Friedrich Wilhelm Nietzsche\\r\\n\\r\\n\\r\\n\\r\\nInhaltsverzeichnis\\r\\n\\r\\n\\r\\n Erst'\n",
      "'er Theil\\r\\n Zarathustra’s Vorrede\\r\\n Die Reden Zarathustra’s\\r\\n Von den drei Verwandlungen\\r\\n Von den Leh'\n",
      "'rstühlen der Tugend\\r\\n Von den Hinterweltlern\\r\\n Von den Verächtern des Leibes\\r\\n Von den Freuden- und L'\n",
      "'eidenschaften\\r\\n Vom bleichen Verbrecher\\r\\n Vom Lesen und Schreiben\\r\\n Vom Baum am Berge\\r\\n Von den Predi'\n",
      "'gern des Todes\\r\\n Vom Krieg und Kriegsvolke\\r\\n Vom neuen Götzen\\r\\n Von den Fliegen des Marktes\\r\\n Von der'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'in Buch für Alle und Keinen\\r\\n\\r\\n\\r\\nvon Friedrich Wilhelm Nietzsche\\r\\n\\r\\n\\r\\n\\r\\nInhaltsverzeichnis\\r\\n\\r\\n\\r\\n Ers'\n",
      "Target data: 'n Buch für Alle und Keinen\\r\\n\\r\\n\\r\\nvon Friedrich Wilhelm Nietzsche\\r\\n\\r\\n\\r\\n\\r\\nInhaltsverzeichnis\\r\\n\\r\\n\\r\\n Erst'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 68 ('i')\n",
      "  expected output: 73 ('n')\n",
      "Step    1\n",
      "  input: 73 ('n')\n",
      "  expected output: 2 (' ')\n",
      "Step    2\n",
      "  input: 2 (' ')\n",
      "  expected output: 31 ('B')\n",
      "Step    3\n",
      "  input: 31 ('B')\n",
      "  expected output: 80 ('u')\n",
      "Step    4\n",
      "  input: 80 ('u')\n",
      "  expected output: 62 ('c')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 144) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           36864     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 144)           147600    \n",
      "=================================================================\n",
      "Total params: 4,122,768\n",
      "Trainable params: 4,122,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([131, 133,  17, 132, 109,  70,  45, 110, 100,  72,  50,  71, 133,\n",
       "        27,  20,   0,  27,  80, 100,  24,  38,  33, 122,  67,  73,  50,\n",
       "       131,   6,  54,  31, 127, 101, 118, 118,  25, 129,  82,  29,  87,\n",
       "        99,   5,  16, 118, 113,  18,  85,  23, 112,  74, 124,  43, 130,\n",
       "        11,   1,  18,  41, 122,  73, 112,  39, 118,  52,  91,  47,  54,\n",
       "         7, 143,  59,  97, 119,  10, 106,  22,  26, 127, 105,  36,  47,\n",
       "        49,  10,  64,  17,   7, 114, 115,   8,  68,  53,  73,  24,  75,\n",
       "        84, 131,   6, 136,  42, 128,  96,  46,  79])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'Werden taucht; die habende, welche in’s\\r\\nWollen und Verlangen _will_:—\\r\\n\\r\\n—die sich selber fliehende'\n",
      "\n",
      "Next Char Predictions: \n",
      " \"ἴὲ2άœkPαêmUlὲ=5\\n=uê9IDπhnUἴ&YBωëκκ:ἄw@~é#1κε3z8δoςNἐ,\\r3LπnδJκWÜRY'„`çλ*ö7;ωôGRT*e2'ζη(iXn9pyἴ&όMἀäQt\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 144)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.9704895\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "289/289 [==============================] - 16s 56ms/step - loss: 2.3640\n",
      "Epoch 2/10\n",
      "289/289 [==============================] - 16s 57ms/step - loss: 1.6918\n",
      "Epoch 3/10\n",
      "289/289 [==============================] - 17s 57ms/step - loss: 1.4766\n",
      "Epoch 4/10\n",
      "289/289 [==============================] - 17s 57ms/step - loss: 1.3797\n",
      "Epoch 5/10\n",
      "289/289 [==============================] - 17s 58ms/step - loss: 1.3185\n",
      "Epoch 6/10\n",
      "289/289 [==============================] - 17s 58ms/step - loss: 1.2725\n",
      "Epoch 7/10\n",
      "289/289 [==============================] - 17s 59ms/step - loss: 1.2342\n",
      "Epoch 8/10\n",
      "289/289 [==============================] - 17s 59ms/step - loss: 1.2002\n",
      "Epoch 9/10\n",
      "289/289 [==============================] - 17s 59ms/step - loss: 1.1690\n",
      "Epoch 10/10\n",
      "289/289 [==============================] - 17s 59ms/step - loss: 1.1395\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            36864     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 144)            147600    \n",
      "=================================================================\n",
      "Total params: 4,122,768\n",
      "Trainable params: 4,122,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: nicht unheil, Flamme und\n",
      "Vergleichung muss, sie sollten die Instinkte *wissen* -- daß du jeder Mangel an einer überwältigten\n",
      "Schritt, ohne dass mir meine Seele in allen ungesetzte Zukünfte.\n",
      "\n",
      "\n",
      "141.\n",
      "\n",
      "Die *Moral der Leibe schneidet* -- -- Das Leben ist ungerecht gefragt: katig, unsäuciglich den Freunde,\n",
      "den Krämern engeln, hob bis sie verbeiste: denne wird ihm Schopenhauerswürdiger: was Alles\n",
      "warn und schliessen schon: die nannte\n",
      "Zarathustra. Geben wir zu fordern hat. - \"Letzteren\n",
      "sind die Begierde nach Namen Mitte an sich ein nur auf Posen“. Wir haben nur\n",
      "wünschen, um seiner Seele auf,\n",
      "sie ist eine Traussigkeit. Die begreifliche Katon entellte *Herdenist*\n",
      "nach Verbheichnist und einen Willen zur Achtung sein wiegel\n",
      "wird, -- : daß es denkbo wild es diese Dipfernd\n",
      "lieben, er siebliche Kraft allein gab mir auch sein und der\n",
      "Macht rolltegnoss ein solches, das vorsichtet werden zu lassen, sondern leugnet sie\n",
      "noch an Scham ist, oder ein Ander- nämlich diese Frage in Gespenster\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
